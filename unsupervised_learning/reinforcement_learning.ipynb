{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4801517",
   "metadata": {},
   "source": [
    "<h2>State Action Value Function (reinforcement learning)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f83e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rewards(num_states, each_step_reward, terminal_left_reward, terminal_right_reward):\n",
    "    rewards = [each_step_reward for _ in range(num_states)]\n",
    "    rewards[0] = terminal_left_reward\n",
    "    rewards[-1] = terminal_right_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317e06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transition_prob(num_states, num_actions, misstep_prob=0):\n",
    "    \n",
    "    p = np.zeros((num_states, num_actions, num_states))\n",
    "    \n",
    "    for i in range(num_states):        \n",
    "        if i != 0:\n",
    "            p[i, 0, i-1] = 1 - misstep_prob\n",
    "            p[i, 1, i-1] = misstep_prob\n",
    "            \n",
    "        if i != num_states - 1:\n",
    "            p[i, 1, i+1] = 1  - misstep_prob\n",
    "            p[i, 0, i+1] = misstep_prob\n",
    " \n",
    "    p[0] = np.zeros((num_actions, num_states))\n",
    "    p[-1] = np.zeros((num_actions, num_states))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255465f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Q_value(num_states, rewards, transition_prob, gamma, V_states, state, action):\n",
    "    q_sa = rewards[state] + gamma * sum([transition_prob[state, action, sp] * V_states[sp] for sp in range(num_states)])\n",
    "    return q_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c51bff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(num_states, rewards, transition_prob, gamma, policy):\n",
    "    max_policy_eval = 10000 \n",
    "    threshold = 1e-10\n",
    "    \n",
    "    V = np.zeros(num_states)\n",
    "    \n",
    "    for i in range(max_policy_eval):\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            v = V[s]\n",
    "            V[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V, s, policy[s])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "                       \n",
    "        if delta < threshold:\n",
    "            break\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b320b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(num_states, num_actions, rewards, transition_prob, gamma, V, policy):\n",
    "    policy_stable = True\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        q_best = V[s]\n",
    "        for a in range(num_actions):\n",
    "            q_sa = calculate_Q_value(num_states, rewards, transition_prob, gamma, V, s, a)\n",
    "            if q_sa > q_best and policy[s] != a:\n",
    "                policy[s] = a\n",
    "                q_best = q_sa\n",
    "                policy_stable = False\n",
    "    \n",
    "    return policy, policy_stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58b21389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_policy(num_states, num_actions, rewards, transition_prob, gamma):\n",
    "    optimal_policy = np.zeros(num_states, dtype=int)\n",
    "    max_policy_iter = 10000 \n",
    "\n",
    "    for i in range(max_policy_iter):\n",
    "        policy_stable = True\n",
    "\n",
    "        V = evaluate_policy(num_states, rewards, transition_prob, gamma, optimal_policy)\n",
    "        optimal_policy, policy_stable = improve_policy(num_states, num_actions, rewards, transition_prob, gamma, V, optimal_policy)\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "            \n",
    "    return optimal_policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3799f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Q_values(num_states, rewards, transition_prob, gamma, optimal_policy):\n",
    "    # Left and then optimal policy\n",
    "    q_left_star = np.zeros(num_states)\n",
    "\n",
    "    # Right and optimal policy\n",
    "    q_right_star = np.zeros(num_states)\n",
    "    \n",
    "    V_star =  evaluate_policy(num_states, rewards, transition_prob, gamma, optimal_policy)\n",
    "\n",
    "    for s in range(num_states):\n",
    "        q_left_star[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V_star, s, 0)\n",
    "        q_right_star[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V_star, s, 1)\n",
    "        \n",
    "    return q_left_star, q_right_star"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
